\section{Managing Groups of Workers}
\label{Sec::WorkerGroups}

When managing a set of workers, a central actor often dispatches requests to a set of workers.
For this purpose, the class \lstinline^actor_pool^ implements a lightweight abstraction for managing a set of workers using a dispatching policy.

Pools are created using the static member function \lstinline^make^, which takes either one argument (the policy) or three (number of workers, factory function for workers, and dispatching policy).
After construction, one can add new workers via messages of the form $('SYS', 'PUT', worker)$, remove workers with $('SYS', 'DELETE', worker)$, and retrieve the set of workers as \lstinline^vector<actor>^ via $('SYS', 'GET')$.
For example, adding \lstinline^worker^ to \lstinline^my_pool^ could be done using \lstinline^send(my_pool, sys_atom::value, put_atom::value, worker)^.

An actor pool takes ownership of its workers.
When forced to quit, it sends an exit messages to all of its workers, forcing them to quit as well.
The pool also monitors all of its workers.

Pools does not cache messages by default, but enqueue them directly in a workers mailbox. Consequently, a terminating worker loses all unprocessed messages.
For more advanced caching strategies, such as reliable message delivery, users can implement their own dispatching policies. 

\subsection{Predefined Dispatching Policies}

The actor pool class comes with a set predefined policies for convenience.

\subsubsection{\lstinline^actor_pool::round_robin^}

This policy forwards incoming requests in a round-robin manner to workers.
There is no guarantee that messages are consumed, i.e., work items are lost if the worker exits before processing all of its messages.

\subsubsection{\lstinline^actor_pool::broadcast^}

This policy forwards \emph{each} message to \emph{all} workers.
Synchronous messages to the pool will be received by all workers, but the client will only recognize the first arriving response message---or error---and discard subsequent messages.
Note that this is not caused by the policy itself, but a consequence of forwarding synchronous messages to more than one actor.

\subsubsection{\lstinline^actor_pool::random^}

This policy forwards incoming requests to one worker from the pool chosen uniformly at random.
Analogous to \lstinline^round_robin^, this policy does not cache or redispatch messages.

\subsection{Example}

\begin{lstlisting}
actor new_worker() {
  return spawn([]() -> behavior {
    return {
      [](int x, int y) {
        return x + y;
      }
    };
  });
}

void broadcast_example() {
  scoped_actor self;
  // spawns a pool with 5 workers
  auto pool5 = [] {
    return actor_pool::make(5, new_worker, actor_pool::broadcast{});
  };
  // spawns a pool with 5 pools with 5 workers each
  auto w = actor_pool::make(5, pool5, actor_pool::broadcast{});
  // will be broadcasted to 25 workers
  self->send(w, 1, 2);
  std::vector<int> results;
  int i = 0;
  self->receive_for(i, 25)(
    [&](int res) {
      results.push_back(res);
    }
  );
  assert(results.size(), 25);
  assert(std::all_of(results.begin(), results.end(),
                     [](int res) { return res == 3; }));
  // terminate pool(s) and all workers
  self->send_exit(w, exit_reason::user_shutdown);
}
\end{lstlisting}
